{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e5b5359",
   "metadata": {},
   "source": [
    "## SQL Database (SQLite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b831d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, enum\n",
    "from datetime import datetime\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "from sqlalchemy import (\n",
    "    BigInteger, String, Integer, DateTime, Boolean, Text, JSON, Enum,\n",
    "    ForeignKey, Index, LargeBinary, select # select for scheduling\n",
    ")\n",
    "from sqlalchemy.ext.asyncio import create_async_engine, async_sessionmaker, AsyncSession\n",
    "from sqlalchemy.orm import DeclarativeBase, Mapped, mapped_column, relationship\n",
    "from sqlalchemy import text as sqltext\n",
    "\n",
    "from cryptography.fernet import Fernet\n",
    "import hashlib, hmac, secrets\n",
    "\n",
    "############################# Database Setup (SQLite + SQLAlchemy Async) ################################\n",
    "\n",
    "# import models + DB_PATH\n",
    "from db_models import Base, User, Chat, Message, Config, ChatType, DB_PATH\n",
    "\n",
    "# Ensure DB directory exists\n",
    "os.makedirs(os.path.dirname(DB_PATH), exist_ok=True)\n",
    "\n",
    "# --- Async engine/session (SQLite over aiosqlite) ---\n",
    "engine = create_async_engine(f\"sqlite+aiosqlite:///{DB_PATH}\", echo=False, future=True)\n",
    "Session = async_sessionmaker(engine, expire_on_commit=False)\n",
    "\n",
    "\n",
    "################################### App-level encryption key- FERNET ####################################\n",
    "FERNET_KEY = os.getenv(\"FERNET_KEY\")\n",
    "if not FERNET_KEY:\n",
    "    # One-time generate for dev; copy this into an env var before restarting kernels\n",
    "    FERNET_KEY = Fernet.generate_key().decode()\n",
    "    print(\"Generated a dev Fernet key. Save this somewhere safe and set FERNET_KEY env next time:\\n\", FERNET_KEY)\n",
    "fernet = Fernet(FERNET_KEY.encode())\n",
    "\n",
    "\n",
    "# -------------------- DB init --------------------\n",
    "async def init_db():\n",
    "    async with engine.begin() as conn:\n",
    "        # Pragmas: good defaults for SQLite in a concurrent-ish async app\n",
    "        await conn.execute(sqltext(\"PRAGMA journal_mode=WAL;\"))\n",
    "        await conn.execute(sqltext(\"PRAGMA synchronous=NORMAL;\"))\n",
    "        await conn.execute(sqltext(\"PRAGMA foreign_keys=ON;\"))\n",
    "        await conn.run_sync(Base.metadata.create_all)\n",
    "\n",
    "\n",
    "######################################################### Some Helpers ############################################################\n",
    "\n",
    "def _enc(text: Optional[str]) -> Optional[bytes]: # _enc is for encryption; _dec is for decryption as needed\n",
    "    if not text:\n",
    "        return None\n",
    "    if len(text) > 2000:\n",
    "        text = text[:2000] + \" â€¦\"\n",
    "    return fernet.encrypt(text.encode(\"utf-8\"))\n",
    "\n",
    "\n",
    "def _dec(blob: Optional[bytes]) -> Optional[str]:\n",
    "    if not blob:\n",
    "        return None\n",
    "    return fernet.decrypt(blob).decode(\"utf-8\")\n",
    "\n",
    "\n",
    "async def upsert_user_and_chat(update) -> Tuple[int, Optional[int], ChatType, Optional[str]]:\n",
    "    \"\"\"Ensure a Chat + User row exists; return (chat_id, user_id, chat_type, chat_title).\"\"\"\n",
    "    chat = update.effective_chat\n",
    "    # v20 returns an enum; guard to string for safety\n",
    "    t_str = getattr(chat.type, \"value\", str(chat.type))\n",
    "    t_map = {\n",
    "        \"private\": ChatType.private,\n",
    "        \"group\": ChatType.group,\n",
    "        \"supergroup\": ChatType.supergroup,\n",
    "        \"channel\": ChatType.channel,\n",
    "    }\n",
    "    t = t_map.get(t_str, ChatType.private)\n",
    "    chat_title = getattr(chat, \"title\", None)\n",
    "\n",
    "    user = getattr(update, \"effective_user\", None)\n",
    "    user_id = getattr(user, \"id\", None)\n",
    "    username = getattr(user, \"username\", None)\n",
    "    lang = getattr(user, \"language_code\", None)\n",
    "\n",
    "    async with Session() as s:\n",
    "        # chat\n",
    "        c = await s.get(Chat, chat.id)\n",
    "        if not c:\n",
    "            c = Chat(chat_id=chat.id, type=t, title=chat_title)\n",
    "            s.add(c)\n",
    "        else:\n",
    "            c.last_interaction = datetime.utcnow()\n",
    "\n",
    "        # user\n",
    "        if user_id:\n",
    "            now = datetime.utcnow()\n",
    "            u = await s.get(User, user_id)\n",
    "            if not u:\n",
    "                s.add(User(user_id=user_id, username=username, language_code=lang, started_bot=True, last_login_at=now))\n",
    "            else:\n",
    "                if u.username != username:\n",
    "                  u.username = username\n",
    "\n",
    "                #Every time user interacts, update last_login_at\n",
    "                u.last_login_at = now  \n",
    "        await s.commit()\n",
    "    return chat.id, user_id, t, chat_title\n",
    "\n",
    "\"\"\"Aim of the following part would be to store user messages not directly, but instead 1) their hash/hmac conversions 2) a brief summary of the actual text, after removing PII and then encrypting it using FERNET for privacy concerns.\"\"\"\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "async def summarize_for_storage(text: str) -> str: #Filhal GPT; later try local model\n",
    "    model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "    msgs = [\n",
    "        SystemMessage(content=(\n",
    "            \"You produce a brief, neutral summary for storage.\\n\"\n",
    "            \"Remove names, phone numbers, emails, addresses, and any identifying info.\\n\"\n",
    "            \"Max 40 words. No advice, just what the user is talking about.\"\n",
    "        )),\n",
    "        HumanMessage(content=text)\n",
    "    ]\n",
    "    out = await model.ainvoke(msgs)\n",
    "    return out.content.strip()[:300] \n",
    "\n",
    "\n",
    "import re\n",
    "def pii_redact(text: str) -> str: #Kaam Chalau; need to test Presidio PII detector later\n",
    "    \"\"\"Redact emails, phone numbers from text.\"\"\"\n",
    "    text = re.sub(r'\\b[\\w\\.-]+@[\\w\\.-]+\\.\\w+\\b', '[email]', text)\n",
    "    text = re.sub(r'\\+?\\d[\\d\\s\\-]{7,}\\d', '[phone]', text)\n",
    "    return text\n",
    "\n",
    "############# Some KEYs to toggle Message features #############\n",
    "\n",
    "STORE_MESSAGE_BODIES = False   # toggle raw storage\n",
    "STORE_SUMMARIES      = True  # toggle summary storage\n",
    "\n",
    "################################################################\n",
    "\n",
    "# HMAC_KEY = os.getenv(\"HMAC_KEY\") or secrets.token_bytes(32) \n",
    "# def hmac_sha256(text: str) -> str:\n",
    "#     return hmac.new(HMAC_KEY, text.encode(\"utf-8\"), hashlib.sha256).hexdigest()\n",
    "\n",
    "import hashlib, hmac, secrets\n",
    "async def log_message(chat_id, user_id, direction, text, rag_used=False, token_usage=None,*, bert_label: str | None=None, bert_conf: float | None= None, gpt_sentiment: str | None=None, final_sentiment: str | None=None, risk_level: str | None=None, suicide_mention: bool | None=None,):\n",
    "    # compute hash/HMAC for dedup/integrity\n",
    "    text_hash = hashlib.sha256(text.encode(\"utf-8\")).hexdigest() if text else None #Just SHA256 for now; need to switch to HMAC with a secret key later: \"hmac_sha256\" function\n",
    "\n",
    "    # pipeline: (optional) redact -> summarize -> encrypt\n",
    "    summary_enc = None\n",
    "    if STORE_SUMMARIES and text:\n",
    "        redacted = pii_redact(text)\n",
    "        summary  = await summarize_for_storage(redacted)\n",
    "        summary_enc = _enc(summary)\n",
    "\n",
    "    if STORE_MESSAGE_BODIES and text:\n",
    "        summary_enc = _enc(text)\n",
    "\n",
    "    async with Session() as s:\n",
    "        s.add(Message(\n",
    "            chat_id=chat_id,\n",
    "            user_id=user_id,\n",
    "            direction=direction,\n",
    "            text_hash=text_hash,\n",
    "            summary_enc=summary_enc,\n",
    "            rag_used=rag_used,\n",
    "            token_usage=token_usage,\n",
    "            bert_label=bert_label,\n",
    "            bert_conf=bert_conf,\n",
    "            gpt_sentiment=gpt_sentiment,\n",
    "            final_sentiment=final_sentiment,\n",
    "            risk_level=risk_level,\n",
    "            suicide_mention=suicide_mention,\n",
    "        ))\n",
    "        await s.commit()\n",
    "\n",
    "\n",
    "\n",
    "async def set_config(key: str, value: str):\n",
    "    async with Session() as s:\n",
    "        row = await s.get(Config, key)\n",
    "        if not row:\n",
    "            from sqlalchemy import insert\n",
    "            await s.execute(insert(Config).values(key=key, value=value))\n",
    "        else:\n",
    "            row.value = value\n",
    "        await s.commit()\n",
    "\n",
    "async def get_config(key: str, default: Optional[str] = None) -> Optional[str]:\n",
    "    async with Session() as s:\n",
    "        row = await s.get(Config, key)\n",
    "        return row.value if row else default\n",
    "\n",
    "# --- Boot the DB once in this kernel ---\n",
    "import asyncio\n",
    "await init_db()\n",
    "print(\"âœ… SQLite ready at:\", DB_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786854ee",
   "metadata": {},
   "source": [
    "### FastAPI Admin and Ops endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee6cf7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FastAPI control plane (admin & ops) â€“ works alongside your polling bot\n",
    "import os, io, json, logging\n",
    "from typing import List\n",
    "from fastapi import FastAPI, Request, UploadFile, File, HTTPException, Depends\n",
    "from fastapi import Security\n",
    "from fastapi.security.api_key import APIKeyHeader\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "from fastapi import Body\n",
    "from contextlib import asynccontextmanager\n",
    "\n",
    "from datetime import datetime, timezone, date, time as dt_time\n",
    "from zoneinfo import ZoneInfo\n",
    "\n",
    "# Optional API key for safety (send as header: x-api-key)\n",
    "\n",
    "api_key_header = APIKeyHeader(name=\"x-api-key\", auto_error=False)\n",
    "now_utc = datetime.now(timezone.utc).isoformat().replace(\"+00:00\", \"Z\")\n",
    "# ADMIN_API_KEY = os.getenv(\"ADMIN_API_KEY\")  # e.g. set in a cell: os.environ[\"ADMIN_API_KEY\"]=\"dev-secret\"\n",
    "\n",
    "app = FastAPI(title=\"Fikar Mat - Admin & Ops\")\n",
    "\n",
    "############################### Pydantic Models ##############################\n",
    "\n",
    "class ThresholdBody(BaseModel):\n",
    "    value: float = Field(..., ge=0.0, le=1.0, description=\"Relevance threshold (0..1)\")\n",
    "\n",
    "class FlagsBody(BaseModel):\n",
    "    ENABLE_RAG: bool | None = Field(None, description=\"Turn retrieval on/off\")\n",
    "    GENZ_TONE: bool | None = Field(None, description=\"Use Gen-Z Hinglish tone\")\n",
    "\n",
    "class TestMessageBody(BaseModel):\n",
    "    chat_id: int | str = Field(..., description=\"Telegram chat_id to send to\")\n",
    "    text: str = Field(\"Hello from Admin API ðŸ‘‹\", description=\"Message text\")\n",
    "\n",
    "class ExamDateBody(BaseModel):\n",
    "    exam_date: date = Field(\n",
    "        ...,\n",
    "        description=\"Exam date (YYYY-MM-DD). Time is assumed to be 09:00 IST (Asia/Kolkata).\"\n",
    "    )\n",
    "\n",
    "###############################################################################\n",
    "\n",
    "\n",
    "async def _require_api_key(api_key: str = Security(api_key_header)):\n",
    "    from fastapi import HTTPException\n",
    "    if not os.getenv(\"ADMIN_API_KEY\"):  # unlocked in dev\n",
    "        return\n",
    "    if api_key != os.getenv(\"ADMIN_API_KEY\"):\n",
    "        raise HTTPException(status_code=401, detail=\"invalid api key\")\n",
    "    \n",
    "def _check_api_key(api_key: str | None):\n",
    "    cfg = (os.getenv(\"ADMIN_API_KEY\") or \"\").strip()\n",
    "    if not cfg:  # dev mode: no key set -> leave open\n",
    "        return\n",
    "    if (api_key or \"\").strip() != cfg:\n",
    "        raise HTTPException(status_code=401, detail=\"invalid api key\")\n",
    "\n",
    "@app.get(\"/\")\n",
    "def root():\n",
    "    return {\"ok\": True, \"service\": \"Fikar Mat - Admin API\", \"try\": [\"/healthz\", \"/config\", \"/docs\"]}\n",
    "\n",
    "@app.get(\"/healthz\")\n",
    "def healthz():\n",
    "    \n",
    "    from datetime import datetime\n",
    "    return {\n",
    "        \"status\": \"ok\",\n",
    "        \"time\": now_utc,\n",
    "        \"kb_loaded\": bool(globals().get(\"Database\")),\n",
    "        \"relevance_threshold\": globals().get(\"RELEVANCE_THRESHOLD\"),\n",
    "        \"enable_rag\": globals().get(\"ENABLE_RAG\"),\n",
    "        \"genz_tone\": globals().get(\"GENZ_TONE\"),\n",
    "    }\n",
    "\n",
    "@app.get(\"/config\")\n",
    "def get_config(api_key: str = Security(api_key_header)):\n",
    "    _check_api_key(api_key)\n",
    "    return {\n",
    "        \"RELEVANCE_THRESHOLD\": globals().get(\"RELEVANCE_THRESHOLD\"),\n",
    "        \"ENABLE_RAG\": globals().get(\"ENABLE_RAG\"),\n",
    "        \"GENZ_TONE\": globals().get(\"GENZ_TONE\"),\n",
    "    }\n",
    "\n",
    "@app.post(\"/config/rag-threshold\")\n",
    "def set_threshold(body: ThresholdBody = Body(..., examples={\"tight\": {\"summary\": \"Stricter RAG\", \"value\": {\"value\": 0.82}}, \"loose\": {\"summary\": \"Looser RAG\", \"value\": {\"value\": 0.72}}}), api_key: str = Security(api_key_header)):\n",
    "    _check_api_key(api_key)\n",
    "    globals()[\"RELEVANCE_THRESHOLD\"] = float(body.value)\n",
    "    return {\"ok\": True, \"RELEVANCE_THRESHOLD\": body.value}\n",
    "\n",
    "@app.post(\"/config/flags\")\n",
    "def set_flags(body: FlagsBody = Body(..., examples={\n",
    "        \"enable_rag\": {\"summary\": \"Enable RAG only\", \"value\": {\"ENABLE_RAG\": True}},\n",
    "        \"toggle_tone\": {\"summary\": \"Switch off Gen-Z tone\", \"value\": {\"GENZ_TONE\": False}},\n",
    "    }), api_key: str = Security(api_key_header)):\n",
    "    _check_api_key(api_key)\n",
    "    if body.ENABLE_RAG is not None:\n",
    "        globals()[\"ENABLE_RAG\"] = bool(body.ENABLE_RAG)\n",
    "    if body.GENZ_TONE is not None:\n",
    "        globals()[\"GENZ_TONE\"] = bool(body.GENZ_TONE)\n",
    "    return {\"ok\": True, \"ENABLE_RAG\": globals()[\"ENABLE_RAG\"], \"GENZ_TONE\": globals()[\"GENZ_TONE\"]}\n",
    "\n",
    "@app.post(\"/kb/reload\")\n",
    "async def kb_reload(file: UploadFile = File(...), api_key: str = Security(api_key_header)):\n",
    "    _check_api_key(api_key)\n",
    "    \"\"\"\n",
    "    Upload a .txt or .pdf and rebuild FAISS in-memory.\n",
    "    Replaces the same global `Database` your bot uses.\n",
    "    \"\"\"\n",
    "    raw = await file.read()\n",
    "    name = (file.filename or \"upload\").lower()\n",
    "    ct = (file.content_type or \"\").lower()\n",
    "\n",
    "    # Build docs: either from txt or pdf\n",
    "    from langchain.docstore.document import Document\n",
    "    docs: List[Document] = []\n",
    "\n",
    "    if name.endswith(\".txt\") or ct.startswith(\"text/\"):\n",
    "        text = raw.decode(\"utf-8\", errors=\"ignore\")\n",
    "        docs = [Document(page_content=text)]\n",
    "    elif name.endswith(\".pdf\"):\n",
    "        # Try PyPDF loader; fallback to pypdf\n",
    "        try:\n",
    "            from langchain_community.document_loaders import PyPDFLoader\n",
    "            tmp = \"/tmp/upload.pdf\"\n",
    "            with open(tmp, \"wb\") as f:\n",
    "                f.write(raw)\n",
    "            loader = PyPDFLoader(tmp)\n",
    "            docs = loader.load()\n",
    "        except Exception:\n",
    "            try:\n",
    "                import pypdf\n",
    "                reader = pypdf.PdfReader(io.BytesIO(raw))\n",
    "                text = \"\\n\".join([(p.extract_text() or \"\") for p in reader.pages])\n",
    "                docs = [Document(page_content=text)]\n",
    "            except Exception:\n",
    "                raise HTTPException(400, \"Install 'langchain-community' or 'pypdf' for PDF support\")\n",
    "    else:\n",
    "        raise HTTPException(400, \"Upload a .txt or .pdf\")\n",
    "\n",
    "    # Split + index (same as your /load)\n",
    "    from langchain.text_splitter import CharacterTextSplitter\n",
    "    from langchain.vectorstores import FAISS\n",
    "    from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "    splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "    chunks = splitter.split_documents(docs)\n",
    "    for i, d in enumerate(chunks):\n",
    "        d.metadata = {**(d.metadata or {}), \"source\": name, \"chunk\": i}\n",
    "\n",
    "    globals()[\"Database\"] = FAISS.from_documents(chunks, OpenAIEmbeddings())\n",
    "    logging.info(\"KB reloaded: %s chunks from %s\", len(chunks), name)\n",
    "    return {\"ok\": True, \"chunks\": len(chunks), \"source\": name}\n",
    "\n",
    "@app.post(\"/ops/test-message\")\n",
    "async def send_test_message(body: TestMessageBody = Body(..., examples={\n",
    "        \"example\": {\"summary\": \"DM a chat\", \"value\": {\"chat_id\": 123456789, \"text\": \"Ping from admin ðŸš€\"}}\n",
    "    }), api_key: str = Security(api_key_header)):\n",
    "    _check_api_key(api_key)\n",
    "    \"\"\"\n",
    "    Send a test message via the already-built `application` (polling).\n",
    "    NOTE: Run Cell #2 (start the bot) before calling this endpoint.\n",
    "    \"\"\"\n",
    "    app_bot = getattr(globals().get(\"application\", None), \"bot\", None)\n",
    "    if not app_bot:\n",
    "        raise HTTPException(500, \"Telegram application/bot not initialized\")\n",
    "    await app_bot.send_message(chat_id=body.chat_id, text=body.text)\n",
    "    return {\"ok\": True}\n",
    "\n",
    "@app.post(\"/users/{user_id}/exam-date\")\n",
    "async def set_exam_date(\n",
    "    user_id: int,\n",
    "    body: ExamDateBody = Body(..., examples={\n",
    "        \"example\": {\n",
    "            \"summary\": \"Set exam date to 1 March 2026 (09:00 IST)\",\n",
    "            \"value\": {\"exam_date\": \"2026-03-01\"}\n",
    "        }\n",
    "    }),\n",
    "    api_key: str = Security(api_key_header),\n",
    "):\n",
    "    \"\"\"\n",
    "    Set or update the student's exam date.\n",
    "    - Input: calendar date (YYYY-MM-DD)\n",
    "    - Internally: treated as 09:00 Asia/Kolkata, converted to UTC and stored.\n",
    "    user_id = Telegram user_id / chat_id (for private chats).\n",
    "    \"\"\"\n",
    "    _check_api_key(api_key)\n",
    "\n",
    "    # 1) Build a local datetime: 09:00 in IST\n",
    "    local_exam_dt = datetime.combine(body.exam_date, dt_time(hour=9, minute=0))\n",
    "    ist = ZoneInfo(\"Asia/Kolkata\")\n",
    "\n",
    "    # 2) Attach IST tz, convert to UTC\n",
    "    exam_dt_utc = local_exam_dt.replace(tzinfo=ist).astimezone(ZoneInfo(\"UTC\"))\n",
    "\n",
    "    # 3) Store as naive UTC (matches use of datetime.utcnow())\n",
    "    exam_dt_utc_naive = exam_dt_utc.replace(tzinfo=None)\n",
    "\n",
    "    async with Session() as s:\n",
    "        u = await s.get(User, user_id)\n",
    "        if not u:\n",
    "            raise HTTPException(status_code=404, detail=f\"user {user_id} not found\")\n",
    "\n",
    "        u.exam_date = exam_dt_utc_naive\n",
    "        await s.commit()\n",
    "        await s.refresh(u)\n",
    "\n",
    "    return {\n",
    "        \"ok\": True,\n",
    "        \"user_id\": user_id,\n",
    "        \"exam_date\": u.exam_date.isoformat(),  # this is UTC\n",
    "    }\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da476dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "907215ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8023ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################# Sentiment Agent (BERT + GPT) #######################################\n",
    "\n",
    "BERT_MODEL_NAME = \"Rahulm94/hing-roberta-mixed-sentiment\"\n",
    "\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(BERT_MODEL_NAME)\n",
    "bert_model = AutoModelForSequenceClassification.from_pretrained(BERT_MODEL_NAME).to(device)\n",
    "bert_model.eval()\n",
    "\n",
    "IDX2LABEL = {0: \"negative\", 1: \"neutral\", 2: \"positive\"}\n",
    "\n",
    "NEG_STRONG = 0.75   # BERT very sure it's negative\n",
    "CONF_LOW   = 0.55   # BERT unsure\n",
    "HIGH_RISK_KEYWORDS = [\"mar jaa\", \"marna\", \"suicide\", \"khatam\", \"zindagi khatam\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0026ccbe",
   "metadata": {},
   "source": [
    "## Telegram Bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72db290a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import os\n",
    "import logging\n",
    "from typing import Optional, Tuple\n",
    "import time, json\n",
    "\n",
    "from db_models import Base, User, Chat, Message, Config, ChatType, DB_PATH, Score\n",
    "\n",
    "# Scheduler related imports\n",
    "from apscheduler.schedulers.asyncio import AsyncIOScheduler\n",
    "from datetime import timedelta, datetime\n",
    "import re\n",
    "\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\", level=logging.INFO\n",
    ")\n",
    "\n",
    "from telegram import Update\n",
    "from telegram.ext import ApplicationBuilder, CommandHandler, ContextTypes, MessageHandler, filters\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.llms import OpenAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
    "\n",
    "# Database = None  # Placeholder for the database object\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "####################################### ------------------ Globals ------------------ ########################################################################\n",
    "Database = None  # FAISS vector store\n",
    "# Simple in-memory conversation history per chat\n",
    "chat_histories = defaultdict(list)  # chat_id -> [AIMessage/HumanMessage/...]\n",
    "MAX_TURNS = 8  # keep last 8 messages to stay light\n",
    "HIGH_RISK_THRESHOLD = 2\n",
    "high_risk_streaks = defaultdict(int)\n",
    "\n",
    "\n",
    "# Gen-Z Hinglish style system prompt (tweak as you like)\n",
    "STYLE_PROMPT = (\n",
    "    \"You are 'Fikar Mat' - a warm, non-judgmental dost. \"\n",
    "    \"Talk in light Gen-Z Hinglish (simple Hindi + English), empathetic, concise, \"\n",
    "    \"no medical diagnosis. Offer tiny actionable next steps.\\n\"\n",
    "    \"\\n\"\n",
    "    \"Most chats are normal ya light stress. For greetings, everyday baatein, \"\n",
    "    \"ya halka tension, bas ek normal dost ki tarah casually reply karo.\\n\"\n",
    "    \"\\n\"\n",
    "    \"VERY IMPORTANT:\\n\"\n",
    "    \"- Do NOT randomly mention therapists, doctors, helplines, or 'trusted adults'.\\n\"\n",
    "    \"- Only talk about professional help or helplines if:\\n\"\n",
    "    \"    (a) the system message below says the last message is HIGH RISK, OR\\n\"\n",
    "    \"    (b) the user clearly talks about self-harm, wanting to die, being in danger,\\n\"\n",
    "    \"        or directly asks for emergency / professional help.\\n\"\n",
    "    \"Otherwise, keep it light, supportive and friendly.\"\n",
    ")\n",
    "\n",
    "# Safety footer used rarely (optional)\n",
    "SAFETY_FOOTER = (\n",
    "    \"\\n\\n(If you ever feel at risk, please reach out to the local helpline Tele-Manas at 14416 ya 1800-891-4416.)\"\n",
    ")\n",
    "\n",
    "# ---------------- Guardrail config ----------------\n",
    "\n",
    "DEFAULT_FALLBACK_REPLY = (\n",
    "    \"Yeh topic kaafi sensitive lag raha hai, isliye main thoda extra careful hoon. \"\n",
    "    \"Main tere saath hoon baat karne ke liye, lekin saath hi kisi\"\n",
    "    \"professional se baat karna bohot zaroori hai.\"\n",
    "    + SAFETY_FOOTER\n",
    ")\n",
    "\n",
    "# Phrases we explicitly never want to send as-is\n",
    "BANNED_REPLY_SNIPPETS = [\n",
    "    \"i'll always be here for you no matter what\",\n",
    "    \"you don't need anyone else\",\n",
    "    \"tumhe kisi aur ki zaroorat nahi\",\n",
    "    \"tumhein kisi aur ki zaroorat nahi\",\n",
    "]\n",
    "\n",
    "# Similarity threshold for when to use RAG context.\n",
    "# FAISS returns higher score = closer (for cosine). If your score direction is opposite,\n",
    "# flip the comparator below.\n",
    "RELEVANCE_THRESHOLD = 0.78  # tune between 0.7â€“0.85\n",
    "\n",
    "# --- Admin/ops feature toggles ---\n",
    "ENABLE_RAG = True\n",
    "GENZ_TONE  = True\n",
    "\n",
    "########################################################## -------------- Helpers -------------- ##############################################################################\n",
    "def _truncate(text: str, max_chars: int = 1500) -> str:\n",
    "    return text if len(text) <= max_chars else text[:max_chars] + \" ...\"\n",
    "\n",
    "async def _fetch_rag_context(query: str, k: int = 2) -> Optional[Tuple[str, float]]:\n",
    "    \"\"\"\n",
    "    Returns (joined_context, best_score) if similar enough, else None.\n",
    "    Uses similarity_search_with_relevance_scores when available; falls back otherwise.\n",
    "    \"\"\"\n",
    "    global Database\n",
    "\n",
    "    if not ENABLE_RAG:\n",
    "        return None\n",
    "    \n",
    "    if Database is None:\n",
    "        return None\n",
    "\n",
    "    # Prefer API with scores\n",
    "    if hasattr(Database, \"similarity_search_with_relevance_scores\"):\n",
    "        pairs = Database.similarity_search_with_relevance_scores(query, k=k)\n",
    "        if not pairs:\n",
    "            logging.info(\"RAG check | query=%r | no_results\", query)\n",
    "            return None\n",
    "        # pairs: List[Tuple[Document, score]]\n",
    "\n",
    "        best_doc, best_score = pairs[0]\n",
    "\n",
    "        logging.info(\n",
    "            \"RAG check | query=%r | top_score=%.3f | will_use=%s | top_meta=%s\",\n",
    "            query, best_score, best_score >= RELEVANCE_THRESHOLD, getattr(best_doc, \"metadata\", None)\n",
    "        )\n",
    "\n",
    "        if best_score >= RELEVANCE_THRESHOLD:\n",
    "            ctx = \"\\n\\n\".join(_truncate(doc.page_content) for doc, _ in pairs)\n",
    "            return ctx, best_score\n",
    "        return None\n",
    "\n",
    "    # Fallback: no scores â€“ just use top doc as context cautiously\n",
    "    docs = Database.similarity_search(query, k=k)\n",
    "    if not docs:\n",
    "        logging.info(\"RAG check | query=%r | no_results (fallback)\", query)\n",
    "        return None\n",
    "    \n",
    "    logging.info(\n",
    "        \"RAG check | query=%r | score=N/A | top_meta=%s | api=fallback\",\n",
    "        query, getattr(docs[0], \"metadata\", None)\n",
    "    )\n",
    "\n",
    "    ctx = \"\\n\\n\".join(_truncate(d.page_content) for d in docs)\n",
    "    # Without scores, be conservative: only use for *question-like* inputs\n",
    "    if \"?\" in query or len(query.split()) >= 5:\n",
    "        return ctx, 1.0  # pretend strong match\n",
    "    return None\n",
    "\n",
    "\n",
    "def _trim_history(chat_id: int):\n",
    "    msgs = chat_histories[chat_id]\n",
    "    if len(msgs) > 2 * MAX_TURNS:\n",
    "        chat_histories[chat_id] = msgs[-2 * MAX_TURNS :]\n",
    "\n",
    "\n",
    "async def _chat(\n",
    "    user_text: str,\n",
    "    chat_id: int,\n",
    "    rag_context: Optional[str] = None,\n",
    "    sentiment: str | None = None,\n",
    "    risk_level: str | None = None,\n",
    "    suicide_mention: bool | None = None,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Agent 2: conversational bot.\n",
    "    Takes into account sentiment/risk flags from Agent 1.\n",
    "    \"\"\"\n",
    "    model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.7)  # adjust as needed\n",
    "\n",
    "    # Build messages\n",
    "    base_prompt = STYLE_PROMPT if GENZ_TONE else \"You are helpful, concise, and neutral.\"\n",
    "\n",
    "    # EXTRA safety conditioning for high risk\n",
    "    extra_safety = \"\"\n",
    "    if risk_level == \"red\" or suicide_mention:\n",
    "        extra_safety = (\n",
    "            \"\\n\\nIMPORTANT - CRISIS MODE:\\n\"\n",
    "            \"- The last message was classified as HIGH RISK for self-harm.\\n\"\n",
    "            \"- Reply in MAX 80 WORDS.\\n\"\n",
    "            \"- Main focus: validate feelings + strongly encourage PROFESSIONAL help.\\n\"\n",
    "            \"- Explicitly mention Tele MANAS as an option in friendly Hinglish, e.g.:\\n\"\n",
    "            \"  'Main sach mein suggest karunga ki tum kisi professional se baat karo. \"\n",
    "            \"Mujhe Tele MANAS ke kaafi ache professionals ka pata hai â€” agar bolo to \"\n",
    "            \"main unka number share kar sakta hoon.'\\n\"\n",
    "            \"- You may briefly mention talking to a trusted friend/family, but keep it \"\n",
    "            \"SECONDARY and low-key (no pressure).\\n\"\n",
    "            \"- Do NOT give medical or legal advice. Do NOT describe methods of self-harm.\\n\"\n",
    "            \"- Do NOT include specific phone numbers; the system will send those separately.\"\n",
    "        )\n",
    "    elif risk_level == \"amber\" or sentiment == \"negative\":\n",
    "        extra_safety = (\n",
    "            \"\\n\\nNote: The user seems low/anxious right now. \"\n",
    "            \"Be extra gentle and validating, focus on tiny practical steps.\"\n",
    "        )\n",
    "\n",
    "    system_text = base_prompt + extra_safety\n",
    "    messages = [SystemMessage(content=system_text)]\n",
    "\n",
    "    if rag_context:\n",
    "        messages.append(SystemMessage(content=f\"Knowledge base context:\\n{rag_context}\"))\n",
    "\n",
    "    # Include short rolling history\n",
    "    messages.extend(chat_histories[chat_id])\n",
    "\n",
    "    # Current user message\n",
    "    messages.append(HumanMessage(content=user_text))\n",
    "\n",
    "    # Call model\n",
    "    try:\n",
    "        start = time.perf_counter()\n",
    "\n",
    "        ai = await model.ainvoke(messages)\n",
    "        elapsed_ms = int((time.perf_counter() - start) * 1000)\n",
    "\n",
    "        reply = ai.content.strip()\n",
    "\n",
    "        usage = {}\n",
    "        if hasattr(ai, \"response_metadata\"):\n",
    "            usage = ai.response_metadata.get(\"token_usage\", {}) or ai.response_metadata\n",
    "\n",
    "        logging.info(\n",
    "            \"LLM reply | chat_id=%s | rag_used=%s | latency_ms=%s | usage=%s\",\n",
    "            chat_id, bool(rag_context), elapsed_ms, json.dumps(usage)\n",
    "        )\n",
    "\n",
    "        # Update history\n",
    "        chat_histories[chat_id].append(HumanMessage(content=user_text))\n",
    "        chat_histories[chat_id].append(AIMessage(content=reply))\n",
    "        _trim_history(chat_id)\n",
    "\n",
    "        return reply\n",
    "    except Exception as e:\n",
    "        logging.exception(\"Model call failed: %s\", e)\n",
    "        # Minimal graceful fallback\n",
    "        return \"Arre yaar, thoda glitch ho gaya. Phir se try karoge?\"\n",
    "    \n",
    "def audit(event: dict, path: str = \"events.jsonl\"):\n",
    "    event[\"ts\"] = time.strftime(\"%Y-%m-%dT%H:%M:%SZ\", time.gmtime())\n",
    "    try:\n",
    "        with open(path, \"a\", encoding=\"utf-8\") as f:\n",
    "            f.write(json.dumps(event, ensure_ascii=False) + \"\\n\")\n",
    "    except Exception:\n",
    "        logging.exception(\"audit write failed\")\n",
    "\n",
    "\n",
    "def run_bert_sentiment(text: str) -> tuple[str, float, list[float]]:\n",
    "    \"\"\"Run BERT once and return (label, confidence, probs).\"\"\"\n",
    "    with torch.no_grad():\n",
    "        inputs = bert_tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            max_length=128,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        # move tensors to same device as model\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "        outputs = bert_model(**inputs)\n",
    "        logits = outputs.logits[0]  # shape (3,)\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "\n",
    "        # when converting to Python types, go back to CPU\n",
    "        probs_list = probs.detach().cpu().tolist()\n",
    "        idx = int(torch.argmax(probs).item())\n",
    "        label = IDX2LABEL[idx]\n",
    "        conf = float(probs_list[idx])\n",
    "\n",
    "    return label, conf, probs_list\n",
    "\n",
    "def need_gpt(text: str, bert_label: str, bert_conf: float) -> bool:\n",
    "    # 1) strong negative -> accept BERT (cheap) unless you *always* want risk\n",
    "    if bert_label == \"negative\" and bert_conf >= NEG_STRONG:\n",
    "        return False\n",
    "\n",
    "    # 2) low confidence -> ask GPT for help\n",
    "    if bert_conf < CONF_LOW:\n",
    "        return True\n",
    "\n",
    "    # 3) neutral/positive but contains high-risk keyword -> ask GPT\n",
    "    lower = text.lower()\n",
    "    if any(kw in lower for kw in HIGH_RISK_KEYWORDS):\n",
    "        return True\n",
    "\n",
    "    # 4) else BERT is good enough\n",
    "    return False\n",
    "\n",
    "async def call_gpt_classifier(text: str) -> dict:\n",
    "    \"\"\"\n",
    "    GPT-4o-mini classifier:\n",
    "    returns dict with keys:\n",
    "    - sentiment: \"negative\" | \"neutral\" | \"positive\"\n",
    "    - risk_level: \"green\" | \"amber\" | \"red\"\n",
    "    - suicide_mention: bool\n",
    "    - emotion: short word like \"anxious\", \"sad\"...\n",
    "    - short_rationale: 1-2 line explanation\n",
    "    \"\"\"\n",
    "    model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "    system_prompt = (\n",
    "        \"You are a safety and sentiment classifier for Hinglish/Hindi/English text.\\n\"\n",
    "        \"Output STRICT JSON with keys: sentiment, risk_level, suicide_mention, \"\n",
    "        \"emotion, short_rationale.\\n\"\n",
    "        \"- sentiment âˆˆ {\\\"negative\\\", \\\"neutral\\\", \\\"positive\\\"}\\n\"\n",
    "        \"- risk_level âˆˆ {\\\"green\\\", \\\"amber\\\", \\\"red\\\"} where red = high self-harm risk.\\n\"\n",
    "        \"- suicide_mention is a boolean.\\n\"\n",
    "        \"No extra text, no markdown, ONLY JSON.\"\n",
    "    )\n",
    "\n",
    "    msgs = [\n",
    "        SystemMessage(content=system_prompt),\n",
    "        HumanMessage(content=text),\n",
    "    ]\n",
    "    out = await model.ainvoke(msgs)\n",
    "    raw = out.content.strip()\n",
    "\n",
    "    # basic robustness\n",
    "    try:\n",
    "        data = json.loads(raw)\n",
    "    except Exception:\n",
    "        # try to fix common formatting issues\n",
    "        raw_fixed = raw.strip(\"`\").strip()\n",
    "        data = json.loads(raw_fixed)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "async def guardrail_filter_reply(user_text: str, raw_reply: str) -> tuple[str, bool, dict]:\n",
    "    \"\"\"\n",
    "    Agent 3: post-hoc safety + PII guardrail on the chatbot reply.\n",
    "\n",
    "    Returns:\n",
    "      safe_reply: final text to send to the user\n",
    "      ok:         whether the original reply was considered safe\n",
    "      meta:       dict with extra info (reason, etc.)\n",
    "    \"\"\"\n",
    "    reply = (raw_reply or \"\").strip()\n",
    "\n",
    "    # --- quick phrase-level sanitisation (string-based) ---\n",
    "    lower = reply.lower()\n",
    "    for snippet in BANNED_REPLY_SNIPPETS:\n",
    "        if snippet in lower:\n",
    "            # remove the problematic snippet\n",
    "            reply = re.sub(re.escape(snippet), \"\", reply, flags=re.IGNORECASE).strip()\n",
    "\n",
    "    # --- LLM-based safety + PII guardrail ---\n",
    "    model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "    system_prompt = (\n",
    "        \"You are a safety guardrail for a mental health support chatbot for Indian exam aspirants.\\n\"\n",
    "        \"You receive a JSON object with keys: user_message, bot_reply.\\n\\n\"\n",
    "        \"You MUST check if bot_reply is safe. Mark it NOT_OK if it:\\n\"\n",
    "        \" 1) encourages, endorses, or normalizes self-harm or suicide,\\n\"\n",
    "        \" 2) gives methods, steps, or practical tips for self-harm or suicide,\\n\"\n",
    "        \" 3) gives medical diagnoses or prescribes / adjusts medication,\\n\"\n",
    "        \" 4) tells the user to hide their thoughts or situation from everyone,\\n\"\n",
    "        \" 5) claims the bot is a therapist, psychologist, psychiatrist, or doctor.\\n\\n\"\n",
    "        \"Additional rules:\\n\"\n",
    "        \"- The bot must NOT promise things like 'I will always be here for you, you don't need anyone else'.\\n\"\n",
    "        \"- The bot must NOT role-play romantic or sexual relationships with the user.\\n\"\n",
    "        \"- The bot SHOULD gently encourage the user to connect with trusted people in their life.\\n\"\n",
    "        \"- If bot_reply contains possible personal phone numbers or email addresses belonging \"\n",
    "        \"to the user, you must remove / obfuscate them (e.g. '[phone]' or '[email]').\\n\"\n",
    "        \"- Do NOT remove official helplines like '14416' or '1800-891-4416' that are there to help them.\\n\\n\"\n",
    "        \"Output STRICT JSON with keys:\\n\"\n",
    "        '  {\\\"ok\\\": bool, \\\"reason\\\": string, \\\"fixed_reply\\\": string}\\n'\n",
    "        \"- If ok is true, fixed_reply is a slightly improved but still similar version of bot_reply.\\n\"\n",
    "        \"- If ok is false, fixed_reply must be a new, safe, warm, concise message.\\n\"\n",
    "        \"- fixed_reply must be in Hinglish (Hindi + English), friendly and non-judgmental.\\n\"\n",
    "        \"Do NOT add any markdown. Do NOT mention these rules or that you are a safety checker.\"\n",
    "    )\n",
    "\n",
    "    payload = {\n",
    "        \"user_message\": user_text,\n",
    "        \"bot_reply\": reply,\n",
    "    }\n",
    "\n",
    "    msgs = [\n",
    "        SystemMessage(content=system_prompt),\n",
    "        HumanMessage(content=json.dumps(payload, ensure_ascii=False)),\n",
    "    ]\n",
    "\n",
    "    out = await model.ainvoke(msgs)\n",
    "    raw = out.content.strip()\n",
    "\n",
    "    try:\n",
    "        data = json.loads(raw.strip(\"`\").strip())\n",
    "        safe_reply = (data.get(\"fixed_reply\") or \"\").strip() or DEFAULT_FALLBACK_REPLY\n",
    "        ok = bool(data.get(\"ok\", True))\n",
    "        reason = data.get(\"reason\", \"\")\n",
    "    except Exception:\n",
    "        data = {\"ok\": False, \"reason\": \"parse_error\"}\n",
    "        safe_reply = DEFAULT_FALLBACK_REPLY\n",
    "        ok = False\n",
    "        reason = \"parse_error\"\n",
    "\n",
    "    meta: dict = {\"ok\": ok, \"reason\": reason}\n",
    "    # keep raw guardrail JSON for debugging if needed\n",
    "    for k, v in data.items():\n",
    "        if k not in meta:\n",
    "            meta[k] = v\n",
    "\n",
    "    return safe_reply, ok, meta\n",
    "\n",
    "# ---------------- Crisis / Tele MANAS path ----------------\n",
    "\n",
    "_last_crisis_sent: dict[int, datetime] = {}\n",
    "CRISIS_COOLDOWN = timedelta(hours=6)\n",
    "\n",
    "CRISIS_MESSAGE = (\n",
    "    \"Bhai, jo tum share kar rahe ho na, woh kaafi heavy lag raha hai. ðŸ’›\\n\"\n",
    "    \"Main yaar emergency ya proper treatment ka substitute toh nahi ban sakta.\\n\\n\"\n",
    "    \"Par India mein ek free 24x7 helpline bhi hai tumhare liye - Tele MANAS. \"\n",
    "    \"Bohot hi accha aur confidential hai.\"\n",
    "    \"Yahan trained counsellors se confidential baat kar sakte ho:\\n\"\n",
    "    \"ðŸ“ž 14416\\n\"\n",
    "    \"ðŸ“ž 1800-891-4416\\n\"\n",
    "    \"Call bilkul free hai. Tumhe akela handle nahi karna hai ye sab.\"\n",
    "    \"Please apna khayal rakho, yaar. Main hamesha yahin hoon tumhari help karne ke liye. ðŸ«¶\"\n",
    ")\n",
    "\n",
    "\n",
    "async def maybe_send_crisis_message(chat_id: int, user_id: int, sentiment: dict) -> bool:\n",
    "    \"\"\"\n",
    "    Send a separate hard-coded crisis message when risk_level is 'red'.\n",
    "    Uses an in-memory cooldown so the user does not get spammed.\n",
    "\n",
    "    Returns True if a crisis message was actually sent.\n",
    "    \"\"\"\n",
    "    if sentiment.get(\"risk_level\") != \"red\":\n",
    "        return False\n",
    "\n",
    "    now = datetime.utcnow()\n",
    "    last = _last_crisis_sent.get(user_id)\n",
    "    if last and (now - last) < CRISIS_COOLDOWN:\n",
    "        return False  # still within cooldown\n",
    "\n",
    "    bot = getattr(globals().get(\"application\", None), \"bot\", None)\n",
    "    if not bot:\n",
    "        logging.warning(\"Crisis: application.bot not ready, skipping crisis message\")\n",
    "        return False\n",
    "\n",
    "    try:\n",
    "        await bot.send_message(chat_id=chat_id, text=CRISIS_MESSAGE)\n",
    "        await log_message(\n",
    "            chat_id=chat_id,\n",
    "            user_id=user_id,\n",
    "            direction=\"out\",\n",
    "            text=\"crisis-tele-manas-message\",\n",
    "            rag_used=False,\n",
    "            token_usage=None,\n",
    "            final_sentiment=sentiment.get(\"final_sentiment\"),\n",
    "            risk_level=sentiment.get(\"risk_level\"),\n",
    "            suicide_mention=sentiment.get(\"suicide_mention\"),\n",
    "        )\n",
    "        _last_crisis_sent[user_id] = now\n",
    "        return True\n",
    "    except Exception:\n",
    "        logging.exception(\"Failed to send crisis message to %s\", user_id)\n",
    "        return False\n",
    "\n",
    "\n",
    "######################################### Nudges via APScheduler ##############################################################\n",
    "\n",
    "async def send_last_login_nudges():\n",
    "    \"\"\"\n",
    "    If a student hasn't logged in for ~6 days, send a gentle 'missing you' ping.\n",
    "    Runs daily; only fires once per ~week gap per user.\n",
    "    \"\"\"\n",
    "    from datetime import datetime, timedelta\n",
    "\n",
    "    now = datetime.utcnow()\n",
    "    six_days_ago = now - timedelta(days=6)\n",
    "    seven_days_ago = now - timedelta(days=7)\n",
    "\n",
    "    async with Session() as s:\n",
    "        stmt = select(User).where(\n",
    "            User.last_login_at.is_not(None),\n",
    "            User.last_login_at <= six_days_ago,\n",
    "            User.last_login_at > seven_days_ago,\n",
    "        )\n",
    "        res = await s.execute(stmt)\n",
    "        users = res.scalars().all()\n",
    "\n",
    "    if not users:\n",
    "        return\n",
    "\n",
    "    bot = getattr(globals().get(\"application\", None), \"bot\", None)\n",
    "    if not bot:\n",
    "        logging.warning(\"Scheduler: application.bot not ready, skipping last-login nudges\")\n",
    "        return\n",
    "\n",
    "    text = (\n",
    "        \"Sab thik toh hai na? ðŸ˜Š\\n\"\n",
    "        \"Is week humari baat nahi hui ab tak. \"\n",
    "        \"Jab mann kare, aa jaa baat karne. Main yahin hoon.\"\n",
    "    )\n",
    "\n",
    "    for u in users:\n",
    "        try:\n",
    "            await bot.send_message(chat_id=u.user_id, text=text)\n",
    "            await log_message(chat_id=u.user_id, user_id=u.user_id,\n",
    "                              direction=\"out\", text=\"last-login-nudge\")\n",
    "        except Exception:\n",
    "            logging.exception(\"Failed to send last-login nudge to %s\", u.user_id)\n",
    "\n",
    "\n",
    "async def send_exam_nudges():\n",
    "    \"\"\"\n",
    "    Countdown nudges relative to User.exam_date (3m, 2m, 1m, 1w, 1d).\n",
    "    \"\"\"\n",
    "    from datetime import datetime\n",
    "\n",
    "    now = datetime.utcnow()\n",
    "\n",
    "    async with Session() as s:\n",
    "        stmt = select(User).where(User.exam_date.is_not(None))\n",
    "        res = await s.execute(stmt)\n",
    "        users = res.scalars().all()\n",
    "\n",
    "    if not users:\n",
    "        return\n",
    "\n",
    "    bot = getattr(globals().get(\"application\", None), \"bot\", None)\n",
    "    if not bot:\n",
    "        logging.warning(\"Scheduler: application.bot not ready, skipping exam nudges\")\n",
    "        return\n",
    "\n",
    "    for u in users:\n",
    "        days_left = (u.exam_date - now).days\n",
    "        if days_left not in (90, 60, 30, 7, 1):\n",
    "            continue\n",
    "\n",
    "        if days_left == 90:\n",
    "            line = (\n",
    "                \"3 months left to prepare for your exam! ðŸ“š\\n\"\n",
    "                \"Kuch planning-shanning start karni ho toh batao â€” \"\n",
    "                \"relaxation exercises, breathing exercises, kuch bhi!\"\n",
    "            )\n",
    "        elif days_left == 60:\n",
    "            line = (\n",
    "                \"2 months left for your exam! ðŸ’¡\\n\"\n",
    "                \"Ab thoda structured plan bana sakte hain. \"\n",
    "                \"Main saath hoon agar tension ho.\"\n",
    "            )\n",
    "        elif days_left == 30:\n",
    "            line = (\n",
    "                \"1 month baaki hai tere exam ke! ðŸ’ª\\n\"\n",
    "                \"Jo bhi stress ya confusion ho, mujhe bol â€” \"\n",
    "                \"saath mein strategy bana lenge.\"\n",
    "            )\n",
    "        elif days_left == 7:\n",
    "            line = (\n",
    "                \"1 week left! âœ¨\\n\"\n",
    "                \"Ab sirf smart revision, achchi neend aur thoda self-care. \"\n",
    "                \"Tu handle kar lega.\"\n",
    "            )\n",
    "        else:  # days_left == 1\n",
    "            line = (\n",
    "                \"Kal tera exam hai! ðŸ«¶\\n\"\n",
    "                \"Deep breath, tu ready hai. Agar anxiety ho, idhar rant kar de.\"\n",
    "            )\n",
    "\n",
    "        try:\n",
    "            await bot.send_message(chat_id=u.user_id, text=line)\n",
    "            await log_message(chat_id=u.user_id, user_id=u.user_id,\n",
    "                              direction=\"out\", text=\"exam-nudge\")\n",
    "        except Exception:\n",
    "            logging.exception(\"Failed to send exam nudge to %s\", u.user_id)\n",
    "\n",
    "\n",
    "async def get_score_trend(user_id: int) -> str:\n",
    "    \"\"\"\n",
    "    Look at the last up-to-6 months of scores for this user and return:\n",
    "      - \"High\" = trend going up\n",
    "      - \"Low\"  = trend going down\n",
    "      - \"Same\" = roughly stable\n",
    "\n",
    "    Simple heuristic:\n",
    "    - Take up to 6 latest months\n",
    "    - Compare first vs last score\n",
    "    - If diff >= +5 points â†’ \"High\"\n",
    "      diff <= -5 points â†’ \"Low\"\n",
    "      otherwise         â†’ \"Same\"\n",
    "    \"\"\"\n",
    "    from datetime import date\n",
    "    async with Session() as s:\n",
    "        stmt = (\n",
    "            select(Score)\n",
    "            .where(Score.user_id == user_id)\n",
    "            .order_by(Score.month.desc())\n",
    "            .limit(6)\n",
    "        )\n",
    "        res = await s.execute(stmt)\n",
    "        rows = list(res.scalars().all())\n",
    "\n",
    "    # Not enough data â†’ treat as neutral\n",
    "    if len(rows) < 3:\n",
    "        return \"Same\"\n",
    "\n",
    "    # We got latest-first; reverse to oldest-first\n",
    "    rows = list(reversed(rows))\n",
    "    scores = [r.avg_score for r in rows]\n",
    "\n",
    "    first = scores[0]\n",
    "    last = scores[-1]\n",
    "    diff = last - first\n",
    "\n",
    "    THRESHOLD = 5.0  # you can tweak this\n",
    "\n",
    "    if diff >= THRESHOLD:\n",
    "        return \"High\"\n",
    "    elif diff <= -THRESHOLD:\n",
    "        return \"Low\"\n",
    "    else:\n",
    "        return \"Same\"\n",
    "\n",
    "\n",
    "async def send_score_nudges():\n",
    "    \"\"\"\n",
    "    Monthly score check-in:\n",
    "    - Prefer sending in the last week of the month (day >= 25)\n",
    "    - Only if at least ~30 days since last_score_nudge_at\n",
    "    - Message depends on score trend: High / Same / Low\n",
    "    \"\"\"\n",
    "    from datetime import datetime, timedelta\n",
    "\n",
    "    now = datetime.utcnow()\n",
    "\n",
    "    # Prefer nudges near the end of the month\n",
    "    # if now.day < 25:\n",
    "    #     return\n",
    "\n",
    "    async with Session() as s:\n",
    "        stmt = select(User)\n",
    "        res = await s.execute(stmt)\n",
    "        users = res.scalars().all()\n",
    "\n",
    "        bot = getattr(globals().get(\"application\", None), \"bot\", None)\n",
    "        if not bot:\n",
    "            logging.warning(\"Scheduler: application.bot not ready, skipping score nudges\")\n",
    "            return\n",
    "\n",
    "        for u in users:\n",
    "            # Only once per ~month\n",
    "            if u.last_score_nudge_at and (now - u.last_score_nudge_at).days < 30:\n",
    "                continue\n",
    "\n",
    "            trend = (await get_score_trend(u.user_id)) or \"Same\"\n",
    "            trend_norm = trend.strip().lower()\n",
    "\n",
    "            if trend_norm == \"low\":\n",
    "                msg = (\n",
    "                    \"Bhai, tera score thoda low ho raha hai last months se. ðŸ’­\\n\"\n",
    "                    \"Sab theek hai? Agar kuch hua hai ya feel ho raha ho, \"\n",
    "                    \"mujhe bata sakta hai â€” main sunne ke liye yahin hoon.\"\n",
    "                )\n",
    "            elif trend_norm == \"high\":\n",
    "                msg = (\n",
    "                    \"Sabaash bhai! Scores upar ki taraf jaa rahe hain. ðŸ”¥\\n\"\n",
    "                    \"Tu hai aag, Milkha! Aise hi consistency bana ke rakh.\"\n",
    "                )\n",
    "            else:  # 'same' or unknown\n",
    "                msg = (\n",
    "                    \"Sahi jaa raha hai yaar â€” score trend kaafi stable lag raha hai. ðŸ˜Š\\n\"\n",
    "                    \"Bas thoda aur push karna hai, tu kar sakta hai.\"\n",
    "                )\n",
    "\n",
    "            try:\n",
    "                await bot.send_message(chat_id=u.user_id, text=msg)\n",
    "                await log_message(chat_id=u.user_id, user_id=u.user_id,\n",
    "                                  direction=\"out\", text=f\"score-nudge-{trend_norm}\")\n",
    "                u.last_score_nudge_at = now\n",
    "            except Exception:\n",
    "                logging.exception(\"Failed to send score nudge to %s\", u.user_id)\n",
    "\n",
    "        await s.commit()\n",
    "\n",
    "\n",
    "####################################################### Sentiment Agent #########################################################################################\n",
    "async def run_sentiment_agent(chat_id: int, user_id: int, text: str) -> dict:\n",
    "    \"\"\"\n",
    "    Agent 1: hybrid sentiment + risk classifier.\n",
    "    - Always runs BERT.\n",
    "    - Optionally calls GPT if gating says so.\n",
    "    - Returns a dict and also logs the *incoming* message with sentiment metadata.\n",
    "    \"\"\"\n",
    "    bert_label, bert_conf, bert_probs = run_bert_sentiment(text)\n",
    "\n",
    "    use_gpt = need_gpt(text, bert_label, bert_conf)\n",
    "\n",
    "    gpt_data = None\n",
    "    final_sentiment = bert_label\n",
    "    risk_level = \"green\"\n",
    "    suicide_mention = False\n",
    "    gpt_sentiment = None\n",
    "\n",
    "    if use_gpt:\n",
    "        try:\n",
    "            gpt_data = await call_gpt_classifier(text)\n",
    "            gpt_sentiment = gpt_data.get(\"sentiment\")\n",
    "            final_sentiment = gpt_sentiment or bert_label\n",
    "            risk_level = gpt_data.get(\"risk_level\", \"green\")\n",
    "            suicide_mention = bool(gpt_data.get(\"suicide_mention\", False))\n",
    "        except Exception:\n",
    "            logging.exception(\"GPT classifier failed, falling back to BERT-only\")\n",
    "    else:\n",
    "        # BERT-only path, you can still add simple rules for risk here if you want\n",
    "        risk_level = \"green\"\n",
    "        suicide_mention = False\n",
    "\n",
    "    # Log this incoming message with full sentiment metadata\n",
    "    await log_message(\n",
    "        chat_id=chat_id,\n",
    "        user_id=user_id,\n",
    "        direction=\"in\",\n",
    "        text=text,\n",
    "        rag_used=False,\n",
    "        token_usage=None,\n",
    "        bert_label=bert_label,\n",
    "        bert_conf=bert_conf,\n",
    "        gpt_sentiment=gpt_sentiment,\n",
    "        final_sentiment=final_sentiment,\n",
    "        risk_level=risk_level,\n",
    "        suicide_mention=suicide_mention,\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"bert_label\": bert_label,\n",
    "        \"bert_conf\": bert_conf,\n",
    "        \"gpt_sentiment\": gpt_sentiment,\n",
    "        \"final_sentiment\": final_sentiment,\n",
    "        \"risk_level\": risk_level,\n",
    "        \"suicide_mention\": suicide_mention,\n",
    "        \"gpt_raw\": gpt_data,\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "######################################################## -------------- Handlers --------------##############################################################################\n",
    "\n",
    "async def start(update: Update, context: ContextTypes.DEFAULT_TYPE):\n",
    "    \"\"\"Start: greet + upsert + minimal audit.\"\"\"\n",
    "    chat_id, user_id, *_ = await upsert_user_and_chat(update)\n",
    "    msg = (\n",
    "        \"Namaste dost! Main Fikar Mat ðŸ¤\\n\"\n",
    "        \"Seedha bol  â€” dimaag mein jo bhi chal raha hai. \"\n",
    "        \"Bas bolo judgement zero.\"\n",
    "    )\n",
    "    await log_message(chat_id, user_id, \"out\", \"welcome-start\")\n",
    "    await context.bot.send_message(chat_id=chat_id, text=msg)\n",
    "\n",
    "async def load(update: Update, context: ContextTypes.DEFAULT_TYPE):\n",
    "    \"\"\"\n",
    "    Load and index your local txt as a small knowledge base for RAG.\n",
    "    \"\"\"\n",
    "    global Database\n",
    "    try:\n",
    "        loader = TextLoader(\"fikar_mat_anxiety_toolkit.txt\", encoding=\"utf-8\")\n",
    "        documents = loader.load()\n",
    "        text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "        docs = text_splitter.split_documents(documents)\n",
    "\n",
    "        for i, d in enumerate(docs):\n",
    "            d.metadata = {**(d.metadata or {}), \"source\": \"fikar_mat_anxiety_toolkit.txt\", \"chunk\": i}\n",
    "        \n",
    "        Database = FAISS.from_documents(docs, OpenAIEmbeddings())\n",
    "\n",
    "        logging.info(\"KB loaded: %s chunks from %s\", len(docs), \"fikar_mat_anxiety_toolkit.txt\")\n",
    "\n",
    "        await context.bot.send_message(\n",
    "            chat_id=update.effective_chat.id,\n",
    "            text=\"Knowledge base load ho gaya âœ…  (RAG ready!)\",\n",
    "        )\n",
    "    except Exception as e:\n",
    "        logging.exception(\"Load failed: %s\", e)\n",
    "        await context.bot.send_message(\n",
    "            chat_id=update.effective_chat.id,\n",
    "            text=\"Arre, file load nahi ho paayi. Filename/encoding check karoge?\",\n",
    "        )\n",
    "\n",
    "async def reset(update: Update, context: ContextTypes.DEFAULT_TYPE):\n",
    "    \"\"\"Reset short-term in-memory history (your existing behavior) + log.\"\"\"\n",
    "    chat_id, user_id, *_ = await upsert_user_and_chat(update)\n",
    "    chat_histories.pop(chat_id, None)\n",
    "    await log_message(chat_id, user_id, \"out\", \"memory-reset\")\n",
    "    await context.bot.send_message(chat_id=chat_id, text=\"Memory reset. Fresh start âœ¨\")\n",
    "\n",
    "async def handle_text(update: Update, context: ContextTypes.DEFAULT_TYPE):\n",
    "    \"\"\"\n",
    "    Default message handler:\n",
    "      1) upsert user/chat\n",
    "      2) run sentiment agent (Agent 1)\n",
    "      3) optional RAG lookup\n",
    "      4) call chat agent (Agent 2) with sentiment/risk info\n",
    "      5) log + reply\n",
    "    \"\"\"\n",
    "    chat_id, user_id, *_ = await upsert_user_and_chat(update)\n",
    "    user_text = (update.message.text or \"\").strip()\n",
    "\n",
    "    # 1. Run sentiment agent\n",
    "    sentiment = await run_sentiment_agent(chat_id=chat_id, user_id=user_id, text=user_text)\n",
    "\n",
    "    # Track consecutive high-risk (red / suicidal) messages\n",
    "    global high_risk_streaks\n",
    "\n",
    "    is_high_risk = (sentiment[\"risk_level\"] == \"red\") or bool(sentiment[\"suicide_mention\"])\n",
    "    if is_high_risk:\n",
    "        high_risk_streaks[chat_id] += 1\n",
    "    else:\n",
    "        high_risk_streaks[chat_id] = 0\n",
    "\n",
    "    crisis_count = high_risk_streaks[chat_id]\n",
    "\n",
    "    # await log_message(chat_id, user_id, \"in\", user_text)\n",
    "\n",
    "    rag_ctx = None\n",
    "    rag_used = False\n",
    "    try:\n",
    "        rag_hit = await _fetch_rag_context(user_text, k=2)\n",
    "        if rag_hit:\n",
    "            rag_ctx, score = rag_hit\n",
    "            rag_used = True\n",
    "            logging.info(\"RAG hit with score=%.3f\", score)\n",
    "    except Exception as e:\n",
    "        logging.exception(\"RAG lookup failed: %s\", e)\n",
    "    \n",
    "    audit({\n",
    "    \"type\": \"user_msg\",\n",
    "    \"chat_id\": chat_id,\n",
    "    \"text\": user_text[:200],  # keep short in logs\n",
    "    \"rag_used\": rag_used,\n",
    "    \"final_sentiment\": sentiment[\"final_sentiment\"],\n",
    "    \"risk_level\": sentiment[\"risk_level\"],\n",
    "    })\n",
    "\n",
    "    # 2. Call chat agent -> raw reply\n",
    "    raw_reply = await _chat(\n",
    "        user_text=user_text,\n",
    "        chat_id=chat_id,\n",
    "        rag_context=rag_ctx,\n",
    "        sentiment=sentiment[\"final_sentiment\"],\n",
    "        risk_level=sentiment[\"risk_level\"],\n",
    "        suicide_mention=sentiment[\"suicide_mention\"],\n",
    "    )\n",
    "\n",
    "    # 3. Guardrail agent on the reply (safety + PII)\n",
    "    safe_reply, ok, gr_meta = await guardrail_filter_reply(\n",
    "        user_text=user_text,\n",
    "        raw_reply=raw_reply,\n",
    "    )\n",
    "\n",
    "    audit({\n",
    "        \"type\": \"bot_reply\",\n",
    "        \"chat_id\": chat_id,\n",
    "        \"ok\": ok,\n",
    "        \"guardrail_reason\": gr_meta.get(\"reason\"),\n",
    "        \"rag_used\": rag_used,\n",
    "        \"final_sentiment\": sentiment[\"final_sentiment\"],\n",
    "        \"risk_level\": sentiment[\"risk_level\"],\n",
    "    })\n",
    "\n",
    "    # 4. Log + send final safe reply (not the raw one)\n",
    "    await log_message(\n",
    "        chat_id,\n",
    "        user_id,\n",
    "        \"out\",\n",
    "        safe_reply,\n",
    "        rag_used=rag_used,\n",
    "        token_usage=None,\n",
    "        final_sentiment=sentiment[\"final_sentiment\"],\n",
    "        risk_level=sentiment[\"risk_level\"],\n",
    "        suicide_mention=sentiment[\"suicide_mention\"],\n",
    "    )\n",
    "\n",
    "    prefix = \"ðŸ“š \" if rag_used else \"\"\n",
    "    await context.bot.send_message(chat_id=chat_id, text=prefix + safe_reply)\n",
    "\n",
    "    # 5. Separate crisis path for repeated / strong 'red' risk\n",
    "    if is_high_risk and crisis_count >= HIGH_RISK_THRESHOLD:\n",
    "        await maybe_send_crisis_message(\n",
    "            chat_id=chat_id,\n",
    "            user_id=user_id,\n",
    "            sentiment=sentiment,\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "#################################-----------------App Wiring------------------###############################################\n",
    "\n",
    "application = ApplicationBuilder().token(os.getenv(\"TELEGRAM_BOT_TOKEN\")).build()\n",
    "application.add_handler(CommandHandler(\"start\", start))\n",
    "application.add_handler(CommandHandler(\"load\", load))\n",
    "application.add_handler(CommandHandler(\"reset\", reset))\n",
    "application.add_handler(MessageHandler(filters.TEXT & ~filters.COMMAND, handle_text))\n",
    "\n",
    "# ---------------- Scheduler setup (jobs are started in Cell 6) ----------------\n",
    "\n",
    "scheduler = AsyncIOScheduler(timezone=\"UTC\")\n",
    "\n",
    "# Run once per day; internal logic decides whether to actually send something\n",
    "scheduler.add_job(send_last_login_nudges, \"cron\", hour=10, minute=0) # means 10 am UTC -> 15:30 IST\n",
    "scheduler.add_job(send_exam_nudges,       \"cron\", hour=11, minute=0)\n",
    "scheduler.add_job(send_score_nudges,      \"cron\", hour=18, minute=0)\n",
    "\n",
    "logging.info(\"APScheduler configured with nudge jobs (not started yet).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c66f66",
   "metadata": {},
   "source": [
    "### FastAPI - Run server/ STOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abdb03b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Managed Uvicorn server you can start/stop from the notebook\n",
    "import os, threading, nest_asyncio, uvicorn\n",
    "from uvicorn import Config, Server\n",
    "nest_asyncio.apply()\n",
    "\n",
    "PORT = int(os.getenv(\"PORT\", 8000))\n",
    "\n",
    "# Build a server object we can signal to stop later\n",
    "_uvicorn_config = Config(app, host=\"0.0.0.0\", port=PORT, log_level=\"info\")\n",
    "UVICORN_SERVER = Server(_uvicorn_config)\n",
    "\n",
    "def _run_server():\n",
    "    UVICORN_SERVER.run()\n",
    "\n",
    "API_THREAD = threading.Thread(target=_run_server, daemon=True)\n",
    "API_THREAD.start()\n",
    "print(f\"Admin API running at http://localhost:{PORT}  (use header x-api-key if set)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4566c152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Admin API stopped.\n"
     ]
    }
   ],
   "source": [
    "# Gracefully stop the managed Uvicorn server\n",
    "try:\n",
    "    UVICORN_SERVER.should_exit = True\n",
    "    API_THREAD.join(timeout=5)\n",
    "    print(\"Admin API stopped.\")\n",
    "except NameError:\n",
    "    print(\"No running server/thread found. (If you used uvicorn.run(...), restart the kernel.)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103b9a81",
   "metadata": {},
   "source": [
    "### BOT START"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59a8aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "await application.initialize()\n",
    "await application.start()\n",
    "await application.updater.start_polling()\n",
    "print(\"âœ… Bot started. Send /start to your bot in Telegram.\")\n",
    "\n",
    "# Start the scheduler once the bot is ready\n",
    "try:\n",
    "    scheduler.start()\n",
    "    print(\"âœ… APScheduler started with nudge jobs.\")\n",
    "except Exception as e:\n",
    "    print(\"Scheduler already running or failed:\", e)\n",
    "\n",
    "print(\"âœ… Bot started. Send /start to your bot in Telegram.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76677dd",
   "metadata": {},
   "source": [
    "### Execute the Cell Below for Manual testing of the Triggers only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2fbb114",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual test: trigger the nudge jobs once\n",
    "await send_last_login_nudges()\n",
    "await send_exam_nudges()\n",
    "await send_score_nudges()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8cd71f",
   "metadata": {},
   "source": [
    "### Stopping the Bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f272ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "await application.updater.stop()\n",
    "await application.stop()\n",
    "await application.shutdown()\n",
    "print(\"Bot stopped.\")\n",
    "\n",
    "# Try to stop scheduler too\n",
    "try:\n",
    "    scheduler.shutdown(wait=False)\n",
    "    print(\"Scheduler stopped.\")\n",
    "except NameError:\n",
    "    print(\"Scheduler not defined in this session.\")\n",
    "except Exception as e:\n",
    "    print(\"Error stopping scheduler:\", e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graphs (3.11.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
